/**
 * Cerebras Provider
 *
 * å¯¦ä½œ Cerebras API çš„å‘¼å«é‚è¼¯
 * Cerebras æä¾›è¶…å¿«é€Ÿçš„ LLM æ¨ç†æœå‹™ï¼ŒAPI æ ¼å¼èˆ‡ OpenAI ç›¸å®¹
 */
import BaseProvider from './BaseProvider.js';

class CerebrasProvider extends BaseProvider {
    constructor() {
        super({
            id: 'cerebras',
            name: 'Cerebras',
            apiKeyStorageKey: 'cerebrasApiKey',
            defaultModel: 'gpt-oss-120b',
            description: 'Cerebras - è¶…å¿«é€Ÿ LLM æ¨ç†å¼•æ“ï¼ŒåŸºæ–¼å°ˆç”¨ç¡¬é«”åŠ é€Ÿã€‚å»ºè­°æ¨¡å‹ï¼šllama3.3-70b, gpt-oss-120b'
        });

        this.apiEndpoint = 'https://api.cerebras.ai/v1/chat/completions';
    }

    /**
     * å‘¼å« Cerebras API
     * @param {string} systemPrompt - System prompt
     * @param {string} userPrompt - User prompt
     * @param {Object} jsonSchema - JSON Schema for structured output
     * @param {Object} options - é¡å¤–é¸é … (model, temperature, etc.)
     * @returns {Promise<Object>} æ¨™æº–åŒ–çš„å›æ‡‰æ ¼å¼
     */
    async callAPI(systemPrompt, userPrompt, jsonSchema, options = {}) {
        const apiKey = await this.getApiKey();

        if (!apiKey) {
            throw new Error(`${this.name} API Key not found. Please set it in Options.`);
        }

        const startTime = Date.now();

        try {
            console.log(`ğŸš€ [NEW ARCHITECTURE] Using ${this.name} Provider (Modular)`);

            // Cerebras ä¸æ”¯æ´ OpenAI çš„ response_formatï¼Œéœ€ä½¿ç”¨ JSON æ¨¡å¼
            // å¦‚æœæä¾›äº† jsonSchemaï¼Œå°‡ schema è³‡è¨ŠåŠ å…¥ system prompt ä¸¦è¦æ±‚ JSON è¼¸å‡º
            let enhancedSystemPrompt = systemPrompt;

            // è¨ˆç®—åˆç†çš„ max_completion_tokens
            // Free tier é™åˆ¶: 60K TPMï¼Œéœ€è€ƒæ…® input + output ç¸½å’Œ
            // é†«ç™‚åˆ†æé€šå¸¸è¼¸å‡º 500-2000 tokensï¼Œè¨­å®š 4096 ä½œç‚ºå®‰å…¨å€¼
            const defaultMaxTokens = 4096;

            const requestBody = {
                model: options.model || this.defaultModel,
                messages: [
                    {
                        role: "system",
                        content: enhancedSystemPrompt
                    },
                    {
                        role: "user",
                        content: userPrompt
                    }
                ],
                stream: false,
                temperature: options.temperature !== undefined ? options.temperature : 1,
                top_p: options.topP !== undefined ? options.topP : 1,
                max_completion_tokens: options.maxCompletionTokens || defaultMaxTokens,
                seed: options.seed !== undefined ? options.seed : 0
            };

            // å¦‚æœæœ‰ jsonSchemaï¼Œå°‡ schema è³‡è¨ŠåŠ å…¥ system prompt
            if (jsonSchema && jsonSchema.schema) {
                enhancedSystemPrompt = `${systemPrompt}\n\nä½ å¿…é ˆå›å‚³ç¬¦åˆä»¥ä¸‹ JSON schema çš„æ ¼å¼ï¼Œåªå›å‚³ JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ï¼š\n${JSON.stringify(jsonSchema.schema, null, 2)}`;
                requestBody.messages[0].content = enhancedSystemPrompt;
            }

            // Cerebras ç‰¹æœ‰åƒæ•¸ï¼šreasoning_effortï¼ˆå¯é¸ï¼‰
            if (options.reasoningEffort) {
                requestBody.reasoning_effort = options.reasoningEffort; // "low", "medium", "high"
            }

            // åŠ å…¥å…¶ä»–é¡å¤–åƒæ•¸
            if (options.additionalParams) {
                Object.assign(requestBody, options.additionalParams);
            }

            // ä¼°ç®—ä¸¦è¨˜éŒ„ Token ç”¨é‡ï¼ˆåœ¨å‘¼å« API å‰ï¼Œä½¿ç”¨æœ€çµ‚çš„ enhancedSystemPromptï¼‰
            this.logTokenEstimation(enhancedSystemPrompt, userPrompt, {
                model: options.model || this.defaultModel
            });

            this.log('API Request', {
                model: options.model || this.defaultModel,
                systemPromptLength: enhancedSystemPrompt.length,
                userPromptLength: userPrompt.length
            });

            const response = await fetch(this.apiEndpoint, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${apiKey}`
                },
                body: JSON.stringify(requestBody)
            });

            if (!response.ok) {
                const errorData = await response.json().catch(() => ({}));

                // ç‰¹åˆ¥è™•ç† 429 Rate Limit éŒ¯èª¤
                if (response.status === 429) {
                    const retryAfter = response.headers.get('Retry-After');
                    const resetTime = response.headers.get('X-RateLimit-Reset');

                    let errorMsg = 'Rate Limit è¶…éé™åˆ¶';
                    if (retryAfter) {
                        errorMsg += `ï¼Œè«‹ç­‰å¾… ${retryAfter} ç§’å¾Œé‡è©¦`;
                    } else if (resetTime) {
                        const resetDate = new Date(parseInt(resetTime) * 1000);
                        errorMsg += `ï¼Œé™åˆ¶å°‡åœ¨ ${resetDate.toLocaleTimeString('zh-TW')} é‡ç½®`;
                    }

                    console.error(`[${this.name}] Rate Limit Details:`, {
                        status: response.status,
                        retryAfter,
                        resetTime,
                        errorData
                    });

                    throw new Error(errorMsg);
                }

                throw new Error(errorData.error?.message || errorData.message || `HTTP error! status: ${response.status}`);
            }

            const data = await response.json();
            const duration = Date.now() - startTime;

            // æå– Rate Limit Headers
            const rateLimitInfo = {
                requests_day_limit: response.headers.get('x-ratelimit-limit-requests-day'),
                tokens_minute_limit: response.headers.get('x-ratelimit-limit-tokens-minute'),
                requests_day_remaining: response.headers.get('x-ratelimit-remaining-requests-day'),
                tokens_minute_remaining: response.headers.get('x-ratelimit-remaining-tokens-minute'),
                requests_day_reset: response.headers.get('x-ratelimit-reset-requests-day'),
                tokens_minute_reset: response.headers.get('x-ratelimit-reset-tokens-minute')
            };

            // è¨˜éŒ„æ•ˆèƒ½æ•¸æ“š
            console.groupCollapsed(`${this.name} API Call (${duration}ms)`);
            console.log("Model:", options.model || this.defaultModel);
            console.log("Token Usage:", data.usage);
            console.log("Rate Limit Status:", {
                'TPM Remaining': `${rateLimitInfo.tokens_minute_remaining} / ${rateLimitInfo.tokens_minute_limit}`,
                'TPM Reset in': `${rateLimitInfo.tokens_minute_reset}s`,
                'RPD Remaining': `${rateLimitInfo.requests_day_remaining} / ${rateLimitInfo.requests_day_limit}`,
                'RPD Reset in': `${rateLimitInfo.requests_day_reset}s`
            });
            console.log("Full Response:", data);
            console.groupEnd();

            return this.formatResponse(data, duration);

        } catch (error) {
            const duration = Date.now() - startTime;
            this.logError(`API call failed after ${duration}ms`, error);
            throw error;
        }
    }

    /**
     * æ ¼å¼åŒ–å›æ‡‰ï¼ˆCerebras ä½¿ç”¨ OpenAI ç›¸å®¹æ ¼å¼ï¼‰
     * @param {Object} rawResponse - Cerebras API åŸå§‹å›æ‡‰
     * @param {number} duration - åŸ·è¡Œæ™‚é–“ï¼ˆæ¯«ç§’ï¼‰
     * @returns {Object} æ¨™æº–åŒ–æ ¼å¼
     */
    formatResponse(rawResponse, duration) {
        return {
            choices: rawResponse.choices,
            usage: rawResponse.usage,
            duration: duration,
            model: rawResponse.model,
            provider: this.id
        };
    }
}

export default CerebrasProvider;
